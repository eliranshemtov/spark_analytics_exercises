{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import math\n",
    "import itertools\n",
    "import os\n",
    "from decimal import Decimal\n",
    "from operator import add\n",
    "from pyspark.sql import Row\n",
    "\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "sqlContext = pyspark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Reading into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts_directory = '/usr/data/movie_scripts/'\n",
    "files_list = os.listdir(scripts_directory)\n",
    "files_list_rdd = sc.parallelize(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actor</th>\n",
       "      <th>movie_name</th>\n",
       "      <th>script</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Al Pacino</td>\n",
       "      <td>The Humbling</td>\n",
       "      <td>Ten minutes to curtain. Ten minutes. All the w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adam Sandler</td>\n",
       "      <td>Mixed Nuts</td>\n",
       "      <td>* [ group singing doo-wop ] * [ doo-wop contin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Al Pacino</td>\n",
       "      <td>Donnie Brasco</td>\n",
       "      <td>You're not saying things that mean anything. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anthony Hopkins</td>\n",
       "      <td>Instinct</td>\n",
       "      <td>- Are you listening? Are you listening to me? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anne Hathaway</td>\n",
       "      <td>Brokeback Mountain</td>\n",
       "      <td>Shit. You pair of deuces lookin' for work... I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Anne Hathaway</td>\n",
       "      <td>Bride Wars</td>\n",
       "      <td>- # I found # - # I found # # So many things #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Anne Hathaway</td>\n",
       "      <td>Colossal</td>\n",
       "      <td>(CRICKETS CHIRPING) (GIRL SPEAKING KOREAN) (WO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>Arnold Schwarzenegger</td>\n",
       "      <td>Collateral Damage</td>\n",
       "      <td>OCD from engine 35. On scene at 902 Sunnyvale....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>Angelina Jolie</td>\n",
       "      <td>Hell's Kitchen</td>\n",
       "      <td>Fuck it! Shut up, all right? Listen, we gotta ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Angelina Jolie</td>\n",
       "      <td>Kung Fu Panda 3</td>\n",
       "      <td>(PO PANTING) Oh-ooh! (GASPING) Stairs. I don't...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>226 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     actor          movie_name  \\\n",
       "0                Al Pacino        The Humbling   \n",
       "1             Adam Sandler          Mixed Nuts   \n",
       "2                Al Pacino       Donnie Brasco   \n",
       "3          Anthony Hopkins            Instinct   \n",
       "4            Anne Hathaway  Brokeback Mountain   \n",
       "..                     ...                 ...   \n",
       "221          Anne Hathaway          Bride Wars   \n",
       "222          Anne Hathaway            Colossal   \n",
       "223  Arnold Schwarzenegger   Collateral Damage   \n",
       "224         Angelina Jolie      Hell's Kitchen   \n",
       "225         Angelina Jolie     Kung Fu Panda 3   \n",
       "\n",
       "                                                script  \n",
       "0    Ten minutes to curtain. Ten minutes. All the w...  \n",
       "1    * [ group singing doo-wop ] * [ doo-wop contin...  \n",
       "2    You're not saying things that mean anything. I...  \n",
       "3    - Are you listening? Are you listening to me? ...  \n",
       "4    Shit. You pair of deuces lookin' for work... I...  \n",
       "..                                                 ...  \n",
       "221  - # I found # - # I found # # So many things #...  \n",
       "222  (CRICKETS CHIRPING) (GIRL SPEAKING KOREAN) (WO...  \n",
       "223  OCD from engine 35. On scene at 902 Sunnyvale....  \n",
       "224  Fuck it! Shut up, all right? Listen, we gotta ...  \n",
       "225  (PO PANTING) Oh-ooh! (GASPING) Stairs. I don't...  \n",
       "\n",
       "[226 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_script_to_meta(filename):\n",
    "    if \"txt\" not in filename:\n",
    "        return []\n",
    "    script = open(scripts_directory + filename, 'r', encoding='utf8').read()\n",
    "    actor, movie = filename[:-4].split(\"_\")\n",
    "    return [Row(actor=actor, movie_name=movie, script=script)]\n",
    "\n",
    "scripts_df = files_list_rdd.flatMap(map_script_to_meta).toDF()\n",
    "scripts_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characters Sanitation and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actor</th>\n",
       "      <th>movie_name</th>\n",
       "      <th>script</th>\n",
       "      <th>tokenized_script</th>\n",
       "      <th>tokens_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Al Pacino</td>\n",
       "      <td>The Humbling</td>\n",
       "      <td>Ten minutes to curtain. Ten minutes. All the w...</td>\n",
       "      <td>[ten, minutes, to, curtain, ten, minutes, all,...</td>\n",
       "      <td>11778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adam Sandler</td>\n",
       "      <td>Mixed Nuts</td>\n",
       "      <td>* [ group singing doo-wop ] * [ doo-wop contin...</td>\n",
       "      <td>[group, singing, doo, wop, doo, wop, continues...</td>\n",
       "      <td>12118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Al Pacino</td>\n",
       "      <td>Donnie Brasco</td>\n",
       "      <td>You're not saying things that mean anything. I...</td>\n",
       "      <td>[you, re, not, saying, things, that, mean, any...</td>\n",
       "      <td>14465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anthony Hopkins</td>\n",
       "      <td>Instinct</td>\n",
       "      <td>- Are you listening? Are you listening to me? ...</td>\n",
       "      <td>[are, you, listening, are, you, listening, to,...</td>\n",
       "      <td>4083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anne Hathaway</td>\n",
       "      <td>Brokeback Mountain</td>\n",
       "      <td>Shit. You pair of deuces lookin' for work... I...</td>\n",
       "      <td>[shit, you, pair, of, deuces, lookin, for, wor...</td>\n",
       "      <td>8189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Anne Hathaway</td>\n",
       "      <td>Bride Wars</td>\n",
       "      <td>- # I found # - # I found # # So many things #...</td>\n",
       "      <td>[i, found, i, found, so, many, things, i, drea...</td>\n",
       "      <td>11401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Anne Hathaway</td>\n",
       "      <td>Colossal</td>\n",
       "      <td>(CRICKETS CHIRPING) (GIRL SPEAKING KOREAN) (WO...</td>\n",
       "      <td>[crickets, chirping, girl, speaking, korean, w...</td>\n",
       "      <td>9671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>Arnold Schwarzenegger</td>\n",
       "      <td>Collateral Damage</td>\n",
       "      <td>OCD from engine 35. On scene at 902 Sunnyvale....</td>\n",
       "      <td>[ocd, from, engine, 35, on, scene, at, 902, su...</td>\n",
       "      <td>5943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>Angelina Jolie</td>\n",
       "      <td>Hell's Kitchen</td>\n",
       "      <td>Fuck it! Shut up, all right? Listen, we gotta ...</td>\n",
       "      <td>[fuck, it, shut, up, all, right, listen, we, g...</td>\n",
       "      <td>6201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Angelina Jolie</td>\n",
       "      <td>Kung Fu Panda 3</td>\n",
       "      <td>(PO PANTING) Oh-ooh! (GASPING) Stairs. I don't...</td>\n",
       "      <td>[po, panting, oh, ooh, gasping, stairs, i, don...</td>\n",
       "      <td>8388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>226 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     actor          movie_name  \\\n",
       "0                Al Pacino        The Humbling   \n",
       "1             Adam Sandler          Mixed Nuts   \n",
       "2                Al Pacino       Donnie Brasco   \n",
       "3          Anthony Hopkins            Instinct   \n",
       "4            Anne Hathaway  Brokeback Mountain   \n",
       "..                     ...                 ...   \n",
       "221          Anne Hathaway          Bride Wars   \n",
       "222          Anne Hathaway            Colossal   \n",
       "223  Arnold Schwarzenegger   Collateral Damage   \n",
       "224         Angelina Jolie      Hell's Kitchen   \n",
       "225         Angelina Jolie     Kung Fu Panda 3   \n",
       "\n",
       "                                                script  \\\n",
       "0    Ten minutes to curtain. Ten minutes. All the w...   \n",
       "1    * [ group singing doo-wop ] * [ doo-wop contin...   \n",
       "2    You're not saying things that mean anything. I...   \n",
       "3    - Are you listening? Are you listening to me? ...   \n",
       "4    Shit. You pair of deuces lookin' for work... I...   \n",
       "..                                                 ...   \n",
       "221  - # I found # - # I found # # So many things #...   \n",
       "222  (CRICKETS CHIRPING) (GIRL SPEAKING KOREAN) (WO...   \n",
       "223  OCD from engine 35. On scene at 902 Sunnyvale....   \n",
       "224  Fuck it! Shut up, all right? Listen, we gotta ...   \n",
       "225  (PO PANTING) Oh-ooh! (GASPING) Stairs. I don't...   \n",
       "\n",
       "                                      tokenized_script  tokens_count  \n",
       "0    [ten, minutes, to, curtain, ten, minutes, all,...         11778  \n",
       "1    [group, singing, doo, wop, doo, wop, continues...         12118  \n",
       "2    [you, re, not, saying, things, that, mean, any...         14465  \n",
       "3    [are, you, listening, are, you, listening, to,...          4083  \n",
       "4    [shit, you, pair, of, deuces, lookin, for, wor...          8189  \n",
       "..                                                 ...           ...  \n",
       "221  [i, found, i, found, so, many, things, i, drea...         11401  \n",
       "222  [crickets, chirping, girl, speaking, korean, w...          9671  \n",
       "223  [ocd, from, engine, 35, on, scene, at, 902, su...          5943  \n",
       "224  [fuck, it, shut, up, all, right, listen, we, g...          6201  \n",
       "225  [po, panting, oh, ooh, gasping, stairs, i, don...          8388  \n",
       "\n",
       "[226 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer,  RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "def tokenize(input_col_name: str, tokenized_col_name: str, output_columns: list, with_count: bool,df):    \n",
    "    tokenizer = Tokenizer(inputCol=input_col_name, outputCol=tokenized_col_name)\n",
    "    count_tokens = udf(lambda words: len(words), IntegerType())\n",
    "    tokenized = tokenizer.transform(df)\n",
    "\n",
    "    regexTokenizer = RegexTokenizer(inputCol=input_col_name, outputCol=tokenized_col_name, pattern=\"\\\\W\")\n",
    "    regexTokenized = regexTokenizer.transform(df)\n",
    "\n",
    "    tokenized = regexTokenized.select(*output_columns)\n",
    "    if with_count:\n",
    "        tokenized = tokenized.withColumn(\"tokens_count\", count_tokens(col(\"tokenized_script\")))\n",
    "    return tokenized\n",
    "\n",
    "tokenized = tokenize(\"script\", \"tokenized_script\", [\"actor\", \"movie_name\", \"script\", \"tokenized_script\"], True, scripts_df)\n",
    "tokenized.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop-Word Sanitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actor</th>\n",
       "      <th>movie_name</th>\n",
       "      <th>script</th>\n",
       "      <th>tokenized_script</th>\n",
       "      <th>tokens_count</th>\n",
       "      <th>sanitized_script</th>\n",
       "      <th>sanitized_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Al Pacino</td>\n",
       "      <td>The Humbling</td>\n",
       "      <td>Ten minutes to curtain. Ten minutes. All the w...</td>\n",
       "      <td>[ten, minutes, to, curtain, ten, minutes, all,...</td>\n",
       "      <td>11778</td>\n",
       "      <td>[ten, minutes, curtain, ten, minutes, world, w...</td>\n",
       "      <td>4956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adam Sandler</td>\n",
       "      <td>Mixed Nuts</td>\n",
       "      <td>* [ group singing doo-wop ] * [ doo-wop contin...</td>\n",
       "      <td>[group, singing, doo, wop, doo, wop, continues...</td>\n",
       "      <td>12118</td>\n",
       "      <td>[group, singing, doo, wop, doo, wop, continues...</td>\n",
       "      <td>6133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Al Pacino</td>\n",
       "      <td>Donnie Brasco</td>\n",
       "      <td>You're not saying things that mean anything. I...</td>\n",
       "      <td>[you, re, not, saying, things, that, mean, any...</td>\n",
       "      <td>14465</td>\n",
       "      <td>[re, saying, things, mean, anything, even, deb...</td>\n",
       "      <td>6557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anthony Hopkins</td>\n",
       "      <td>Instinct</td>\n",
       "      <td>- Are you listening? Are you listening to me? ...</td>\n",
       "      <td>[are, you, listening, are, you, listening, to,...</td>\n",
       "      <td>4083</td>\n",
       "      <td>[listening, listening, yes, one, gorilla, see,...</td>\n",
       "      <td>1786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anne Hathaway</td>\n",
       "      <td>Brokeback Mountain</td>\n",
       "      <td>Shit. You pair of deuces lookin' for work... I...</td>\n",
       "      <td>[shit, you, pair, of, deuces, lookin, for, wor...</td>\n",
       "      <td>8189</td>\n",
       "      <td>[shit, pair, deuces, lookin, work, suggest, ge...</td>\n",
       "      <td>3969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Anne Hathaway</td>\n",
       "      <td>Bride Wars</td>\n",
       "      <td>- # I found # - # I found # # So many things #...</td>\n",
       "      <td>[i, found, i, found, so, many, things, i, drea...</td>\n",
       "      <td>11401</td>\n",
       "      <td>[found, found, many, things, dreamed, dreamed,...</td>\n",
       "      <td>5425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Anne Hathaway</td>\n",
       "      <td>Colossal</td>\n",
       "      <td>(CRICKETS CHIRPING) (GIRL SPEAKING KOREAN) (WO...</td>\n",
       "      <td>[crickets, chirping, girl, speaking, korean, w...</td>\n",
       "      <td>9671</td>\n",
       "      <td>[crickets, chirping, girl, speaking, korean, w...</td>\n",
       "      <td>4832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>Arnold Schwarzenegger</td>\n",
       "      <td>Collateral Damage</td>\n",
       "      <td>OCD from engine 35. On scene at 902 Sunnyvale....</td>\n",
       "      <td>[ocd, from, engine, 35, on, scene, at, 902, su...</td>\n",
       "      <td>5943</td>\n",
       "      <td>[ocd, engine, 35, scene, 902, sunnyvale, six, ...</td>\n",
       "      <td>3033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>Angelina Jolie</td>\n",
       "      <td>Hell's Kitchen</td>\n",
       "      <td>Fuck it! Shut up, all right? Listen, we gotta ...</td>\n",
       "      <td>[fuck, it, shut, up, all, right, listen, we, g...</td>\n",
       "      <td>6201</td>\n",
       "      <td>[fuck, shut, right, listen, gotta, get, roll, ...</td>\n",
       "      <td>2775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Angelina Jolie</td>\n",
       "      <td>Kung Fu Panda 3</td>\n",
       "      <td>(PO PANTING) Oh-ooh! (GASPING) Stairs. I don't...</td>\n",
       "      <td>[po, panting, oh, ooh, gasping, stairs, i, don...</td>\n",
       "      <td>8388</td>\n",
       "      <td>[po, panting, oh, ooh, gasping, stairs, think,...</td>\n",
       "      <td>4378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>226 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     actor          movie_name  \\\n",
       "0                Al Pacino        The Humbling   \n",
       "1             Adam Sandler          Mixed Nuts   \n",
       "2                Al Pacino       Donnie Brasco   \n",
       "3          Anthony Hopkins            Instinct   \n",
       "4            Anne Hathaway  Brokeback Mountain   \n",
       "..                     ...                 ...   \n",
       "221          Anne Hathaway          Bride Wars   \n",
       "222          Anne Hathaway            Colossal   \n",
       "223  Arnold Schwarzenegger   Collateral Damage   \n",
       "224         Angelina Jolie      Hell's Kitchen   \n",
       "225         Angelina Jolie     Kung Fu Panda 3   \n",
       "\n",
       "                                                script  \\\n",
       "0    Ten minutes to curtain. Ten minutes. All the w...   \n",
       "1    * [ group singing doo-wop ] * [ doo-wop contin...   \n",
       "2    You're not saying things that mean anything. I...   \n",
       "3    - Are you listening? Are you listening to me? ...   \n",
       "4    Shit. You pair of deuces lookin' for work... I...   \n",
       "..                                                 ...   \n",
       "221  - # I found # - # I found # # So many things #...   \n",
       "222  (CRICKETS CHIRPING) (GIRL SPEAKING KOREAN) (WO...   \n",
       "223  OCD from engine 35. On scene at 902 Sunnyvale....   \n",
       "224  Fuck it! Shut up, all right? Listen, we gotta ...   \n",
       "225  (PO PANTING) Oh-ooh! (GASPING) Stairs. I don't...   \n",
       "\n",
       "                                      tokenized_script  tokens_count  \\\n",
       "0    [ten, minutes, to, curtain, ten, minutes, all,...         11778   \n",
       "1    [group, singing, doo, wop, doo, wop, continues...         12118   \n",
       "2    [you, re, not, saying, things, that, mean, any...         14465   \n",
       "3    [are, you, listening, are, you, listening, to,...          4083   \n",
       "4    [shit, you, pair, of, deuces, lookin, for, wor...          8189   \n",
       "..                                                 ...           ...   \n",
       "221  [i, found, i, found, so, many, things, i, drea...         11401   \n",
       "222  [crickets, chirping, girl, speaking, korean, w...          9671   \n",
       "223  [ocd, from, engine, 35, on, scene, at, 902, su...          5943   \n",
       "224  [fuck, it, shut, up, all, right, listen, we, g...          6201   \n",
       "225  [po, panting, oh, ooh, gasping, stairs, i, don...          8388   \n",
       "\n",
       "                                      sanitized_script  sanitized_count  \n",
       "0    [ten, minutes, curtain, ten, minutes, world, w...             4956  \n",
       "1    [group, singing, doo, wop, doo, wop, continues...             6133  \n",
       "2    [re, saying, things, mean, anything, even, deb...             6557  \n",
       "3    [listening, listening, yes, one, gorilla, see,...             1786  \n",
       "4    [shit, pair, deuces, lookin, work, suggest, ge...             3969  \n",
       "..                                                 ...              ...  \n",
       "221  [found, found, many, things, dreamed, dreamed,...             5425  \n",
       "222  [crickets, chirping, girl, speaking, korean, w...             4832  \n",
       "223  [ocd, engine, 35, scene, 902, sunnyvale, six, ...             3033  \n",
       "224  [fuck, shut, right, listen, gotta, get, roll, ...             2775  \n",
       "225  [po, panting, oh, ooh, gasping, stairs, think,...             4378  \n",
       "\n",
       "[226 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "def remove_stop_words(input_col_name: str, sanitized_col_name: str, output_columns: list, with_count: bool,df):\n",
    "    stop_word_remover = StopWordsRemover(inputCol=input_col_name, outputCol=sanitized_col_name)\n",
    "    sanitized = stop_word_remover.transform(df)\n",
    "    sanitized = sanitized.select(*output_columns)\n",
    "    if with_count:\n",
    "        count_tokens = udf(lambda words: len(words), IntegerType())\n",
    "        sanitized = sanitized.withColumn(\"sanitized_count\", count_tokens(col(\"sanitized_script\")))\n",
    "    return sanitized\n",
    "\n",
    "sanitized = remove_stop_words(\"tokenized_script\", \"sanitized_script\", [\"actor\", \"movie_name\", \"script\", \"tokenized_script\", \"tokens_count\", \"sanitized_script\"], True, tokenized)\n",
    "sanitized.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Only What Really Matters\n",
    "And adding an ID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>actor</th>\n",
       "      <th>movie_name</th>\n",
       "      <th>sanitized_script</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Al Pacino</td>\n",
       "      <td>The Humbling</td>\n",
       "      <td>[ten, minutes, curtain, ten, minutes, world, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Adam Sandler</td>\n",
       "      <td>Mixed Nuts</td>\n",
       "      <td>[group, singing, doo, wop, doo, wop, continues...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Al Pacino</td>\n",
       "      <td>Donnie Brasco</td>\n",
       "      <td>[re, saying, things, mean, anything, even, deb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Anthony Hopkins</td>\n",
       "      <td>Instinct</td>\n",
       "      <td>[listening, listening, yes, one, gorilla, see,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Anne Hathaway</td>\n",
       "      <td>Brokeback Mountain</td>\n",
       "      <td>[shit, pair, deuces, lookin, work, suggest, ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>222</td>\n",
       "      <td>Anne Hathaway</td>\n",
       "      <td>Bride Wars</td>\n",
       "      <td>[found, found, many, things, dreamed, dreamed,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>223</td>\n",
       "      <td>Anne Hathaway</td>\n",
       "      <td>Colossal</td>\n",
       "      <td>[crickets, chirping, girl, speaking, korean, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>224</td>\n",
       "      <td>Arnold Schwarzenegger</td>\n",
       "      <td>Collateral Damage</td>\n",
       "      <td>[ocd, engine, 35, scene, 902, sunnyvale, six, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>225</td>\n",
       "      <td>Angelina Jolie</td>\n",
       "      <td>Hell's Kitchen</td>\n",
       "      <td>[fuck, shut, right, listen, gotta, get, roll, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>226</td>\n",
       "      <td>Angelina Jolie</td>\n",
       "      <td>Kung Fu Panda 3</td>\n",
       "      <td>[po, panting, oh, ooh, gasping, stairs, think,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>226 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                  actor          movie_name  \\\n",
       "0      1              Al Pacino        The Humbling   \n",
       "1      2           Adam Sandler          Mixed Nuts   \n",
       "2      3              Al Pacino       Donnie Brasco   \n",
       "3      4        Anthony Hopkins            Instinct   \n",
       "4      5          Anne Hathaway  Brokeback Mountain   \n",
       "..   ...                    ...                 ...   \n",
       "221  222          Anne Hathaway          Bride Wars   \n",
       "222  223          Anne Hathaway            Colossal   \n",
       "223  224  Arnold Schwarzenegger   Collateral Damage   \n",
       "224  225         Angelina Jolie      Hell's Kitchen   \n",
       "225  226         Angelina Jolie     Kung Fu Panda 3   \n",
       "\n",
       "                                      sanitized_script  \n",
       "0    [ten, minutes, curtain, ten, minutes, world, w...  \n",
       "1    [group, singing, doo, wop, doo, wop, continues...  \n",
       "2    [re, saying, things, mean, anything, even, deb...  \n",
       "3    [listening, listening, yes, one, gorilla, see,...  \n",
       "4    [shit, pair, deuces, lookin, work, suggest, ge...  \n",
       "..                                                 ...  \n",
       "221  [found, found, many, things, dreamed, dreamed,...  \n",
       "222  [crickets, chirping, girl, speaking, korean, w...  \n",
       "223  [ocd, engine, 35, scene, 902, sunnyvale, six, ...  \n",
       "224  [fuck, shut, right, listen, gotta, get, roll, ...  \n",
       "225  [po, panting, oh, ooh, gasping, stairs, think,...  \n",
       "\n",
       "[226 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, lit, row_number\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "# It could be just that way but because monotonically_increasing_id() produces unique values that might be huge,\n",
    "# its much simpler on the eye to use \"row id\" as following\n",
    "# sanitized = sanitized.select(monotonically_increasing_id().alias('id'), \"actor\", \"movie_name\", \"sanitized_script\")\n",
    "\n",
    "\n",
    "sanitized = sanitized.withColumn(\"temp_lit\",lit(\"ABC\"))\n",
    "w = Window().partitionBy('temp_lit').orderBy(lit('A'))\n",
    "sanitized = sanitized.withColumn(\"id\", row_number().over(w)).drop(\"temp_lit\")\n",
    "sanitized = sanitized.select(\"id\" ,\"actor\", \"movie_name\", \"sanitized_script\")\n",
    "sanitized.toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating An Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs a list of dicts, of the following form: \n",
      " [{'tell': [0], 'hawaii': [0], 'unbelievable': [0], 'oh': [0], 'yeah': [0], 'well': [0]},'happened': [1], 'met': [1], 'guy': [1], 'best': [1], 'week': [1], 'life': [1]...\n"
     ]
    }
   ],
   "source": [
    "def create_index (row):\n",
    "    index = {}\n",
    "    for token in row[3]:\n",
    "        if row[0] not in index.get(token, []):\n",
    "            if index.get(token):\n",
    "                index[token].append(row[0])\n",
    "            else:\n",
    "                index[token] = [row[0]]\n",
    "    return index\n",
    "\n",
    "indexes_per_doc = sanitized.rdd.map(create_index)\n",
    "print(f\"outputs a list of dicts, of the following form: \\n \"\n",
    "          \"[{'tell': [0], 'hawaii': [0], 'unbelievable': [0], 'oh': [0], 'yeah': [0], 'well': [0]},\"\n",
    "              \"'happened': [1], 'met': [1], 'guy': [1], 'best': [1], 'week': [1], 'life': [1]...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ten', [1]),\n",
       " ('minutes', [1]),\n",
       " ('curtain', [1]),\n",
       " ('world', [1]),\n",
       " ('stage', [1]),\n",
       " ('men', [1]),\n",
       " ('women', [1]),\n",
       " ('merely', [1]),\n",
       " ('players', [1]),\n",
       " ('exits', [1])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened = indexes_per_doc.flatMap(lambda doc: (doc.items()))\n",
    "flattened.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>everything</td>\n",
       "      <td>[1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sometimes</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>[1, 3, 5, 6, 7, 8, 9, 10, 11, 13, 14, 16, 17, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>step</td>\n",
       "      <td>[1, 3, 4, 7, 8, 10, 11, 16, 17, 18, 19, 21, 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>destroyed</td>\n",
       "      <td>[1, 2, 18, 22, 26, 27, 29, 31, 33, 37, 38, 39,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37722</th>\n",
       "      <td>sunbathe</td>\n",
       "      <td>[216]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37723</th>\n",
       "      <td>repentant</td>\n",
       "      <td>[217, 220]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37724</th>\n",
       "      <td>uric</td>\n",
       "      <td>[220]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37725</th>\n",
       "      <td>vegetarianism</td>\n",
       "      <td>[220]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37726</th>\n",
       "      <td>laurence</td>\n",
       "      <td>[221]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37727 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               token                                               docs\n",
       "0         everything  [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...\n",
       "1          sometimes  [1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15, ...\n",
       "2                 10  [1, 3, 5, 6, 7, 8, 9, 10, 11, 13, 14, 16, 17, ...\n",
       "3               step  [1, 3, 4, 7, 8, 10, 11, 16, 17, 18, 19, 21, 23...\n",
       "4          destroyed  [1, 2, 18, 22, 26, 27, 29, 31, 33, 37, 38, 39,...\n",
       "...              ...                                                ...\n",
       "37722       sunbathe                                              [216]\n",
       "37723      repentant                                         [217, 220]\n",
       "37724           uric                                              [220]\n",
       "37725  vegetarianism                                              [220]\n",
       "37726       laurence                                              [221]\n",
       "\n",
       "[37727 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index = flattened.reduceByKey(lambda a, b: a+b)\n",
    "inverted_index.toDF([\"token\", \"docs\"]).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF\n",
    "### Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sometimes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>step</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>destroyed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37722</th>\n",
       "      <td>sunbathe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37723</th>\n",
       "      <td>repentant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37724</th>\n",
       "      <td>uric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37725</th>\n",
       "      <td>vegetarianism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37726</th>\n",
       "      <td>laurence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37727 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               token\n",
       "0         everything\n",
       "1          sometimes\n",
       "2                 10\n",
       "3               step\n",
       "4          destroyed\n",
       "...              ...\n",
       "37722       sunbathe\n",
       "37723      repentant\n",
       "37724           uric\n",
       "37725  vegetarianism\n",
       "37726       laurence\n",
       "\n",
       "[37727 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = inverted_index.toDF([\"token\", \"docs\"]).select(\"token\")\n",
    "all_tokens.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf looks like this (not dispalying actual content because its huge!): \n",
      "[(1,\n",
      "  {'ten': 4,\n",
      "   'minutes': 8,\n",
      "   'curtain': 1,\n",
      "   'world': 5,\n",
      "   'stage': 15,\n",
      "   'men': 7,\n",
      "   'women': 8,\n",
      "   'merely': 2,\n",
      "   'players': 4,...\n"
     ]
    }
   ],
   "source": [
    "def gather_tf(data):\n",
    "    tf = {}\n",
    "    for term in data.sanitized_script:\n",
    "        tf[term] = data.sanitized_script.count(term)\n",
    "    return (data.id, tf)\n",
    "\n",
    "tf = sanitized.rdd.map(gather_tf)\n",
    "print(\"tf looks like this (not dispalying actual content because its huge!): \\n\"\n",
    "     \"\"\"[(1,\n",
    "  {'ten': 4,\n",
    "   'minutes': 8,\n",
    "   'curtain': 1,\n",
    "   'world': 5,\n",
    "   'stage': 15,\n",
    "   'men': 7,\n",
    "   'women': 8,\n",
    "   'merely': 2,\n",
    "   'players': 4,...\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the first 10 tokens in the full tokens list: ['sometimes', '10', 'step', 'destroyed', 'led', 'depressed', 'solve', 'healthy', 'whimpering']\n"
     ]
    }
   ],
   "source": [
    "all_tokens_list = all_tokens.rdd.flatMap(lambda x: x).collect()\n",
    "print(f\"Here's the first 10 tokens in the full tokens list: {all_tokens_list[1:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_bc = sc.broadcast(all_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_huge_tuple(row):\n",
    "    word_count_list = [('doc_id', [row[0]])]\n",
    "    for token in all_tokens_bc.value:\n",
    "        word_count_list.append((token, [row[1].get(token,0)]))\n",
    "    return word_count_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The full_vocabulary_tf_per_doc looks like this (not displaying all of it because of it's size): \n",
      "\n",
      "[[('doc_id', [1]),\n",
      "  ('everything', [10]),\n",
      "  ('sometimes', [3]),\n",
      "  ('10', [2]),\n",
      "  ('step', [1]),\n",
      "  ('destroyed', [1]),\n",
      "  ('led', [1]),\n",
      "  ('depressed', [0]),\n",
      "  ('solve', [0]),\n",
      "  ('healthy', [0]),\n",
      "  ('whimpering', [0]),\n",
      "  ('horrible', [0]),\n",
      "  ('orange', [0]),...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_vocabulary_tf_per_doc = tf.map(map_to_huge_tuple)\n",
    "print(\"\"\" The full_vocabulary_tf_per_doc looks like this (not displaying all of it because of it's size): \\n\n",
    "[[('doc_id', [1]),\n",
    "  ('everything', [10]),\n",
    "  ('sometimes', [3]),\n",
    "  ('10', [2]),\n",
    "  ('step', [1]),\n",
    "  ('destroyed', [1]),\n",
    "  ('led', [1]),\n",
    "  ('depressed', [0]),\n",
    "  ('solve', [0]),\n",
    "  ('healthy', [0]),\n",
    "  ('whimpering', [0]),\n",
    "  ('horrible', [0]),\n",
    "  ('orange', [0]),...\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vocabulary_tf_per_doc = full_vocabulary_tf_per_doc.flatMap(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = full_vocabulary_tf_per_doc.filter(lambda x: x[0] == 'doc_id').reduceByKey(lambda a, b: a + b).collect()\n",
    "doc_ids_list = doc_ids[0][1]\n",
    "doc_ids_list = [str(i) for i in doc_ids_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_rdd = full_vocabulary_tf_per_doc.reduceByKey(lambda a, b: a + b).map(lambda x: (x[0], *x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_table = tf_rdd.toDF(['token'] + doc_ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>everything</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sometimes</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>step</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>destroyed</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37723</th>\n",
       "      <td>sunbathe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37724</th>\n",
       "      <td>repentant</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37725</th>\n",
       "      <td>uric</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37726</th>\n",
       "      <td>vegetarianism</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37727</th>\n",
       "      <td>laurence</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37728 rows × 227 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               token   1  2   3  4  5  6  7  8  9  ...  217  218  219  220  \\\n",
       "0         everything  10  6  11  0  1  5  9  7  4  ...    5    8    4    7   \n",
       "1          sometimes   3  1   4  1  2  2  0  3  0  ...    2    1    3    1   \n",
       "2                 10   2  0   1  0  1  1  1  3  1  ...   12    4    1    2   \n",
       "3               step   1  0   2  1  0  0  1  1  0  ...    1    2    0    3   \n",
       "4          destroyed   1  1   0  0  0  0  0  0  0  ...    0    0    0    0   \n",
       "...              ...  .. ..  .. .. .. .. .. .. ..  ...  ...  ...  ...  ...   \n",
       "37723       sunbathe   0  0   0  0  0  0  0  0  0  ...    0    0    0    0   \n",
       "37724      repentant   0  0   0  0  0  0  0  0  0  ...    1    0    0    1   \n",
       "37725           uric   0  0   0  0  0  0  0  0  0  ...    0    0    0    1   \n",
       "37726  vegetarianism   0  0   0  0  0  0  0  0  0  ...    0    0    0    2   \n",
       "37727       laurence   0  0   0  0  0  0  0  0  0  ...    0    0    0    0   \n",
       "\n",
       "       221  222  223  224  225  226  \n",
       "0        4    8   10    3    5    9  \n",
       "1        1    5    0    0    2    1  \n",
       "2        3    4    1    2    1    1  \n",
       "3        2    1    0    0    1    2  \n",
       "4        1    0    0    0    0    2  \n",
       "...    ...  ...  ...  ...  ...  ...  \n",
       "37723    0    0    0    0    0    0  \n",
       "37724    0    0    0    0    0    0  \n",
       "37725    0    0    0    0    0    0  \n",
       "37726    0    0    0    0    0    0  \n",
       "37727    1    0    0    0    0    0  \n",
       "\n",
       "[37728 rows x 227 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_table.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>docs_count</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>everything</td>\n",
       "      <td>219</td>\n",
       "      <td>0.026907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sometimes</td>\n",
       "      <td>165</td>\n",
       "      <td>0.308547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>152</td>\n",
       "      <td>0.390097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>step</td>\n",
       "      <td>126</td>\n",
       "      <td>0.576348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>destroyed</td>\n",
       "      <td>40</td>\n",
       "      <td>1.706963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37722</th>\n",
       "      <td>sunbathe</td>\n",
       "      <td>1</td>\n",
       "      <td>4.727388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37723</th>\n",
       "      <td>repentant</td>\n",
       "      <td>2</td>\n",
       "      <td>4.321923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37724</th>\n",
       "      <td>uric</td>\n",
       "      <td>1</td>\n",
       "      <td>4.727388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37725</th>\n",
       "      <td>vegetarianism</td>\n",
       "      <td>1</td>\n",
       "      <td>4.727388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37726</th>\n",
       "      <td>laurence</td>\n",
       "      <td>1</td>\n",
       "      <td>4.727388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37727 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               token  docs_count       idf\n",
       "0         everything         219  0.026907\n",
       "1          sometimes         165  0.308547\n",
       "2                 10         152  0.390097\n",
       "3               step         126  0.576348\n",
       "4          destroyed          40  1.706963\n",
       "...              ...         ...       ...\n",
       "37722       sunbathe           1  4.727388\n",
       "37723      repentant           2  4.321923\n",
       "37724           uric           1  4.727388\n",
       "37725  vegetarianism           1  4.727388\n",
       "37726       laurence           1  4.727388\n",
       "\n",
       "[37727 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def counter(row):\n",
    "    c = 0\n",
    "    for i in row[1]:\n",
    "        if i != 0:\n",
    "            c += 1\n",
    "    return row[0], c\n",
    "\n",
    "\n",
    "idf_table = inverted_index.map(counter).map(lambda x: (x[0], x[1], math.log(226/(1+x[1])))).toDF([\"token\", \"docs_count\", \"idf\"])\n",
    "idf_table.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>docs_count</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970s</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.727388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.727388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57th</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.727388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.727388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>829</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.727388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37722</th>\n",
       "      <td>wack</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.811097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37723</th>\n",
       "      <td>wane</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.727388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37724</th>\n",
       "      <td>weed</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>2.647946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37725</th>\n",
       "      <td>widening</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.727388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37726</th>\n",
       "      <td>ymca</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.727388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37727 rows × 229 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          token    1    2    3    4    5    6    7    8    9  ...  219  220  \\\n",
       "0         1970s  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1           296  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "2          57th  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "3           675  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "4           829  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "...         ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "37722      wack  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "37723      wane  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "37724      weed  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "37725  widening  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "37726      ymca  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "       221  222  223  224  225  226  docs_count       idf  \n",
       "0      0.0  0.0  0.0  0.0  0.0  0.0           1  4.727388  \n",
       "1      0.0  0.0  0.0  0.0  0.0  0.0           1  4.727388  \n",
       "2      0.0  0.0  0.0  0.0  0.0  0.0           1  4.727388  \n",
       "3      0.0  0.0  0.0  0.0  0.0  0.0           1  4.727388  \n",
       "4      0.0  0.0  0.0  0.0  0.0  0.0           1  4.727388  \n",
       "...    ...  ...  ...  ...  ...  ...         ...       ...  \n",
       "37722  0.0  0.0  0.0  0.0  0.0  0.0           4  3.811097  \n",
       "37723  0.0  0.0  0.0  0.0  0.0  0.0           1  4.727388  \n",
       "37724  0.0  0.0  0.0  0.0  0.0  0.0          15  2.647946  \n",
       "37725  0.0  0.0  0.0  0.0  0.0  0.0           1  4.727388  \n",
       "37726  0.0  0.0  0.0  0.0  0.0  0.0           1  4.727388  \n",
       "\n",
       "[37727 rows x 229 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_table = tf_table.join(idf_table, [\"token\"])\n",
    "\n",
    "\n",
    "def calculate_tfidf(row):\n",
    "    result = [row[0]]\n",
    "    for i in range(1, len(row)-2):\n",
    "        result.append(row[i]*row[-1])\n",
    "    result.extend(row[-2:])\n",
    "    return result\n",
    "\n",
    "tfidf_table = tfidf_table.rdd.map(calculate_tfidf).toDF(['token'] + doc_ids_list + [\"docs_count\", \"idf\"])\n",
    "tfidf_table.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    dot_product = 0\n",
    "    v1i_sqr = 0\n",
    "    v2i_sqr = 0\n",
    "    \n",
    "    for i in range(len(v1)):\n",
    "        dot_product += v1[i] * v2[i]\n",
    "    \n",
    "    for i in range(len(v1)):\n",
    "        v1i_sqr += v1[i] ** 2\n",
    "        v2i_sqr += v2[i] ** 2\n",
    "    \n",
    "    v1i_sqr = sqrt(v1i_sqr)\n",
    "    v2i_sqr = sqrt(v2i_sqr)\n",
    "    \n",
    "    return dot_product / (v1i_sqr * v2i_sqr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "column_names = tfidf_table.schema.names[1:-4]\n",
    "vectors_list = []\n",
    "for column_name in column_names:\n",
    "    vectors_list.append(Vectors.dense(tfidf_table.select(column_name).rdd.map(lambda x: x[0]).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_cos_similarity_for_every_doc():\n",
    "    max_similarity_per_doc = []\n",
    "    for i,v1 in enumerate(vectors_list):\n",
    "        top_5_list = []\n",
    "        for j,v2 in enumerate(vectors_list):\n",
    "            if i==j: continue\n",
    "            cos = cosine_similarity(v1, v2)\n",
    "            if len(top_5_list) < 5 or any([cos > tup[0] for tup in top_5_list]):\n",
    "                top_5_list.append((cos, i, j))\n",
    "            if len(top_5_list) > 5: top_5_list.remove(min(top_5_list))\n",
    "        max_similarity_per_doc.append(top_5_list)\n",
    "    return max_similarity_per_doc\n",
    "\n",
    "max_similarity_per_doc = get_max_cos_similarity_for_every_doc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a taste of the highest similarities of each doc: \n",
      " [(0.07878699169387886, 0, 49), (0.07468789964132368, 0, 60), (0.052398588254967, 0, 100), (0.07571166555522885, 0, 162), (0.0523730593905043, 0, 173), (0.1333927725566076, 1, 72), (0.11232127827208745, 1, 76), (0.08447551046251867, 1, 92), (0.08916196906111647, 1, 110), (0.19917297488347485, 1, 218), (0.19880983714233572, 2, 29), (0.059779197390971976, 2, 69)]\n"
     ]
    }
   ],
   "source": [
    "all_highest_similarities = []\n",
    "for top5_of_doc in max_similarity_per_doc:\n",
    "        all_highest_similarities += top5_of_doc\n",
    "print(f\"Here is a taste of the highest similarities of each doc: \\n {all_highest_similarities[0:12]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 108, 204), (0.9989728021326493, 100, 173), (0.9771074450258495, 181, 205), (0.9747325030154703, 132, 205), (0.9705073954735131, 132, 181)]\n"
     ]
    }
   ],
   "source": [
    "def find_top_5():\n",
    "    sorted_similarities = sorted(all_highest_similarities, key=lambda x: x[0], reverse=True)\n",
    "    top_10_with_duplicates = sorted_similarities[:10]\n",
    "    # Since every similarity will appear twice (permutaions of a couple), we just take the even-indexed similarities\n",
    "    return [sim for i, sim in enumerate(top_10_with_duplicates) if i%2 ==0]\n",
    "print(find_top_5())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An alternative method that brings less data to the driver\n",
    "### I chose to stick with the first method as it was already ran (and it took the time 🕓)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    import pyspark.sql.functions as func\n",
    "\n",
    "    def cosine_similarity_alternative(df, col1, col2):\n",
    "        df_cosine = df.select(func.sum(df[col1] * df[col2]).alias('dot'), \n",
    "                              func.sqrt(func.sum(df[col1]**2)).alias('norm1'), \n",
    "                              func.sqrt(func.sum(df[col2] **2)).alias('norm2'))\n",
    "        d = df_cosine.rdd.collect()[0].asDict()\n",
    "        return d['dot']/(d['norm1'] * d['norm2'])\n",
    "\n",
    "    results = []\n",
    "    for i in range(1, 15):\n",
    "        for j in range(1, 15):\n",
    "            if i == j: continue\n",
    "            results.append(cosine_similarity_alternative(tfidf_table, str(i), str(j)))\n",
    "    results\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "def search(search_term: str, k_results = 10):\n",
    "    results = []\n",
    "    search_vector = prepare_for_search(search_term)\n",
    "    for i, v2 in enumerate(vectors_list):\n",
    "        results.append((i, cosine_similarity(search_vector, v2)))\n",
    "    results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "    return results[:k_results]\n",
    "    \n",
    "def prepare_for_search(search_term: str):  \n",
    "    cleaned = clean(search_term)\n",
    "    return vectorize(cleaned)\n",
    "    \n",
    "def clean(search_term):    \n",
    "    term = search_term.split(\" \")\n",
    "    term_df = sc.parallelize(term).map(lambda x: [x]).toDF([\"text\"])\n",
    "    tokenized_search = tokenize(\"text\", \"tokens\", [\"tokens\"], False, term_df)\n",
    "    cleared_term = remove_stop_words(\"tokens\", \"clear_text\", [\"clear_text\"], False, tokenized_search)\n",
    "    cleared_term = cleared_term.rdd.map(lambda x: x[0]).filter(lambda x: len(x) != 0).flatMap(lambda list: list)\n",
    "    return cleared_term.collect()\n",
    "\n",
    "def vectorize(cleaned_term: list):\n",
    "    result = tfidf_table.withColumn(\"score\", F.when((tfidf_table.token.isin(cleaned_term)), F.col(\"idf\")).otherwise(0))\n",
    "    result = result.select(\"score\")\n",
    "    return Vectors.dense(result.rdd.map(lambda x: x[0]).collect())\n",
    "    \n",
    "    \n",
    "    \n",
    "ten_terms  = {\n",
    "    \"pacino_1\" : \"\"\"Look at me. Look at the kite. Jack... Oh, I've seen that outfit before. I can't remember when. - Last night. - Last night! These things do return. God, my head is still pounding like a drum. You party hard. Yeah, well, you don't look worse than I am. I didn't drink as much as you. History of my life, Sara. Psychiatrist by day, party animal by night. I thought you've told me you were a lawyer. I'm not, yet. Excuse me. - Yeah. - Hey, Jack. It's Shelly. - I've been calling all morning. You OK? - I'm Ok. What's up? I was worried. You worry too much, Shelly. What's going on? Frank Parks is been calling. He needs to speak with you immediately. Pass him through. I had fun last night, Miss. Pollard. Me too. Thanks for the wine tasting, Dr. Gramm. Jack. Hey, Jack, I've got Frank Parks. Go ahead. - Hey, Frank. - Jack. We got another one. Oh, no. The same? Every detail. Right down to the lateral laceration. It's the Seattle slayer again. - Where are you now? - Look, Jack. There's a tape. You're gonna wanna see it. Jack, are you still there? Yeah. Meet me at my office, okay? Sure, Jack. Where to? 114 Western, please. Quickly. Welcome back, Seattle. We're continuing our interview with John Forster... who was convicted for the death of Janie Kay, nine years ago... based on the eyewitness testimony of her twin sister, Janie Kay. John Forster was dated to die at midnight, at Walla Walla Penitentiary. - Do you mind changing it? - John Forster. What's your reaction to the State... Thank you. Idiot! Congratulations! For what? I was referring to the hottie that you left with the party last night. She was a quite piece of ass. - Remember her name? - Yes, I do. Sara Pollard. Is that a cut on your nose? You didn't have that before I left. I fell out of the bed. FBI hit here yet? - Yeah, 2 minutes in the conference room. - Okay. Any calls? New York Times, Washington times, Newsweek. They all wanna know if you have a quote about Forster's execution. - What else is new? - Kim Cummings called twice. - She was worried about you too. - Kim knows this procedure. Apparently not.\"\"\",\n",
    "    \"amy_adams_2\" : \"\"\"Play one of the best new FPS shooters, search Steam for PROJECT WARLOCK RADIO ANNOUNCER: All news, all the time. This is WINS. You give us 22 minutes, we'll give you the world. REPORTER: Good evening. It's 42 degrees at 5:00 and here's what's happening. Mayor Koch urges the talks to resume between the city and unions, but he says the unions have to give in a bit. The PBA negotiator says he wants real salary increases. Nassau police ask an 18-year-old mother why she abandoned her child. A federal judge rules an accused war criminal must testify in his deportation hearing. A House committee tells the president it does not like proposed Mideast arms sales. The news watch never stops! This is WINS. WINS news time, 5:02. Contract talks have ended temporarily between the city and its labor union with both sides making some angry charges. ANNOUNCER: The municipal labor unions broke off talks demanding that the city withdraw what they call their outrageous and irresponsible contract proposals. The Patrolmen's Benevolent Association and the Uniformed Firefighters Association are the two unions not participating in the coalition bargaining with the city, and the PBA chief negotiator says he wants some solid gains for the men in blue. A bankrupt New York City can't afford to lose money, but city officials say it did, between two and five million dollars in the first two months of 1978 alone... (A HORSE WITH NO NAME BY AMERICA PLAYING) ♪ On the first part of the journey I was lookin' at all the life ♪ There were plants and birds and rocks and things ♪ There was sand and hills and rings ♪ The first thing I met was a fly with a buzz ♪ And the sky with no clouds ♪ The heat was hot and the ground was dry but the air was full of sound ♪ CARL: I do all kinds of business with him. He knows Carl Elway. He knows exactly what he's getting into. \"\"\",\n",
    "    \"jolie_3\" : \"\"\"Hrothgar! Hrothgar! Hrothgar! Hrothgar! Hrothgar! Hrothgar! I want mead! Give me some mead, my Queen! Thank you, my beautiful Queen. Hrothgar! Hrothgar! Hrothgar! This is how it works, Aesher. After you die, you wouldn't really be dead providing you have accepted him as the one and only God. All right, back! Back! Here, my beauty, give me a kiss. I want a kiss! Give me a kiss! I want a kiss! Please, stop it! More! My thanes, my beautiful thanes! One year ago, I, Hrothgar, your King swore that we would celebrate our victories in a new hall, mighty and beautiful! Have I not kept my oath? Yeah. In this hall, we shall divide the spoils of our conquests, the gold and the treasure. And this shall be a place of merriment, joy, and fornication! From now until the end of time, I name this hall Herot! Treasure! Let's hand out some treasure! Give me some of that! From my conquests! Unferth! For Unferth, for Unferth, my wisest advisor, violator of virgins and best and bravest of brave brawlers. Unferth, where the hell are you, you weasel-faced bastard? I'm here, my King. Unferth, come here, you ungrateful lout! Hrothgar! Hrothgar! Hrothgar! Hrothgar! Hrothgar! Hrothgar! He faced a demon dragon When other men would freeze And then, my lords, he took his sword And brought it to its knees... Hrothgar! Hrothgar! The greatest of our kings He broke the dragon's wings Hrothgar! Hrothgar! The kingdom fell in darkness And shadows ruled the night With no sign of dawn, he soldiered on And brought us back to life Hrothgar! Hrothgar! He never shook your faith Hrothgar! Hrothgar! Let every cup be raised Hrothgar! Hrothgar! He offered us protection When monsters roamed the land And one by one, he took them on They perished at his hand Mead! Mead! Mead! You're spilling it. Where's my mead? You're spilling it. You're spilling it! Cain, you clumsy idiot! How dare you waste the King's mead? He rose up like a savior When every hope was gone The beast was gored and peace restored His memory will live on Hrothgar! Hrothgar! Let every cup be raised Hrothgar! Hrothgar! Now and forever A sword! Give me a sword! Come! Arm yourselves! Stay down, my Lady! - Give me a sword! A sword! - My Lord! No! Fight me! Fight me! Fight me. You fight me, damn you. Nay. What was that? Grendel. Grendel, what have you done? What have you done, Grendel? Fish and wolf and bear and sheep or two, ac nan men. Men, Grendel. They have slain so many of our kind. Was Hrothgar there?\"\"\",\n",
    "    \"hathaway_4\" : \"\"\"Charles, you have finally lost your senses. This venture is impossible. For some. Gentlemen, the only way to achieve the impossible is to believe it is possible. That kind of thinking could ruin you. I'm willing to take that chance. Imagine trading posts in Rangoon, Bangkok, Jakarta... The nightmare again? I won't be long. I'm falling down a dark hole, then I see strange creatures. What kind of creatures? Well, there's a dodo bird, a rabbit in a waistcoat, a smiling cat. I didn't know cats could smile. Neither did I. And there's a blue caterpillar. Blue caterpillar. Do you think I've gone round the bend? I'm afraid so. You're mad, bonkers, off your head. But I'll tell you a secret. All the best people are. It's only a dream, Alice. Nothing can harm you there. But if you get too frightened, you can always wake up. Like this. Ow! Must we go? Doubt they'll notice if we never arrive. They will notice. Where's your corset? And no stockings. I'm against them. But you're not properly dressed. Who's to say what is proper? What if it was agreed that \"proper\" was wearing a codfish on your head? - Would you wear it? - Alice. To me, a corset is like a codfish. Please, not today. Father would have laughed. I'm sorry. I'm tired. I didn't sleep well last night. Did you have bad dreams again? Only one. It's always the same, ever since I can remember. Do you think that's normal? Don't most people have different dreams? I don't know. There. You're beautiful. Now, can you manage a smile? At last. We thought you'd never arrive. Alice, Hamish is waiting to dance with you. Go. You do realize it's well past 4:00. Now everything will have to be rushed through. - I am sorry. - Oh, never mind! Forgive my wife. She's been planning this affair for over 20 years. If only Charles were here... My condolences. I think of your husband often. He was truly a man of vision. I hope you don't think I've taken advantage of your misfortunes. Of course not. I'm pleased that you purchased the company. I was a fool for not investing in his mad venture when I had the chance. Charles thought so, too. Hamish, do you ever tire of quadrille? On the contrary. I find it invigorating.\"\"\",\n",
    "    \"hopkins_5\" : \"\"\"Some people hear their own inner voices... with great clearness... and they live by what they hear. Such people become crazy... or they become legends. Tristan Ludlow was born in the Moon of the Falling Leaves. It was a terrible winter. His mother almost died bringing him into this world. His father, the Colonel, brought him to me. I wrapped him in a bear skin and held him all that night. As he grew into a man... I taught him the great joy of the kill... when the hunter cuts out its warm heart and hold sit in his hands... setting its spirit free. Colonel Ludlow had three sons... but Tristan was his favorite. I had had sons too. But they were gone now... forever. It was a very bad time. The Colonel had tried to help the People... but it was no use. So he decided to go his own way. He wanted to lose the madness over the mountains, he said... and begin again. \"Lose the madness\", he said. And so we lived for many years... and the boys grew strong. Alfred was the older brother... old even for his years. Samuel was the youngest. There was nothing these brothers would not do for him. They watched over him like a treasure. One year-- I am an old man and cannot remember the year. But it was the Moon of the Red Grass... when Isabel Ludlow, their mother... went away for the winter. She said the winters were too cruel for her. She said she was afraid of the bears. She was a strange woman anyway. That spring, though, she did not return. And, after that, she did not come much to see us. Alfred wrote her many letters... but Tristan refused to speak of her. His world was here with me. Every warrior hopes a good death will find him... but Tristan couldn't wait. He went looking for his. Tristan! Here. - Was it a bear? - Yes, sir. - Can you breathe? \"\"\",\n",
    "    \n",
    "    \"autocad_wiki_page_6\" : \"\"\"AutoCAD is a commercial computer-aided design (CAD) \"\"\",\n",
    "    \"ynet_hebrew_article_7\" : \"\"\" בימים האחרונים חודשו כאמור הפרחות הבלונים וזוהו עשרות בלוני נפץ שהופרחו לעבר עוטף עזה. הבוקר אותר צרור בלונים עם רימון יד מאולתר באזור קיבוץ ניר עוז שבחבל אשכול. לפי רשות הכיבוי, ביום חמישי פרצו לפחות שלוש שריפות באזור מועצת אשכול בשל בלוני תבערה. מרשות הטבע והגנים נמסר כי כ-300 דונמים עלו באש בשמורת באר .אחרי התקיפה כתב שר הביטחון בני גנץ בטוויטר: \"מדינת ישראל לא תקבל שום הפרת ריבונות ופגיעה בתושבי\"\"\",\n",
    "    \"data_mining_wiki_8\" : \"\"\"Data mining is a process of discovering patterns in large data sets \"\"\",\n",
    "    \"sharp_pc_wiki9\" : \"\"\" The Sharp PC-E500S was a 1995 pocket computer by Sharp Corporation \"\"\",\n",
    "    \"simple\": \"my name is eliran shem tov\"\n",
    "}\n",
    "\n",
    "ten_results = []\n",
    "for term in ten_terms.values():\n",
    "    ten_results.append(search(term, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(search_id=1, doc_id=167, actor='Angelina Jolie'),\n",
       " Row(search_id=1, doc_id=130, actor='Anne Hathaway'),\n",
       " Row(search_id=1, doc_id=184, actor='Anne Hathaway'),\n",
       " Row(search_id=1, doc_id=136, actor='Anthony Hopkins'),\n",
       " Row(search_id=1, doc_id=139, actor='Arnold Schwarzenegger'),\n",
       " Row(search_id=1, doc_id=179, actor='Angelina Jolie'),\n",
       " Row(search_id=1, doc_id=125, actor='Angelina Jolie'),\n",
       " Row(search_id=1, doc_id=21, actor='Al Pacino'),\n",
       " Row(search_id=1, doc_id=165, actor='Anne Hathaway'),\n",
       " Row(search_id=1, doc_id=93, actor='Arnold Schwarzenegger'),\n",
       " Row(search_id=2, doc_id=130, actor='Anne Hathaway'),\n",
       " Row(search_id=2, doc_id=77, actor='Al Pacino'),\n",
       " Row(search_id=2, doc_id=50, actor='Arnold Schwarzenegger'),\n",
       " Row(search_id=2, doc_id=9, actor='Angelina Jolie'),\n",
       " Row(search_id=2, doc_id=85, actor='Anne Hathaway'),\n",
       " Row(search_id=2, doc_id=179, actor='Angelina Jolie'),\n",
       " Row(search_id=2, doc_id=101, actor='Al Pacino'),\n",
       " Row(search_id=2, doc_id=164, actor='Amy Adams'),\n",
       " Row(search_id=2, doc_id=180, actor='Amy Adams'),\n",
       " Row(search_id=2, doc_id=169, actor='Adam Sandler'),\n",
       " Row(search_id=3, doc_id=167, actor='Angelina Jolie'),\n",
       " Row(search_id=3, doc_id=22, actor='Al Pacino'),\n",
       " Row(search_id=3, doc_id=79, actor='Anne Hathaway'),\n",
       " Row(search_id=3, doc_id=28, actor='Arnold Schwarzenegger'),\n",
       " Row(search_id=3, doc_id=10, actor='Amy Adams'),\n",
       " Row(search_id=3, doc_id=142, actor='Anthony Hopkins'),\n",
       " Row(search_id=3, doc_id=8, actor='Al Pacino'),\n",
       " Row(search_id=3, doc_id=60, actor='Amy Adams'),\n",
       " Row(search_id=3, doc_id=147, actor='Adam Sandler'),\n",
       " Row(search_id=3, doc_id=64, actor='Anne Hathaway'),\n",
       " Row(search_id=4, doc_id=110, actor='Angelina Jolie'),\n",
       " Row(search_id=4, doc_id=98, actor='Arnold Schwarzenegger'),\n",
       " Row(search_id=4, doc_id=116, actor='Al Pacino'),\n",
       " Row(search_id=4, doc_id=95, actor='Adam Sandler'),\n",
       " Row(search_id=4, doc_id=6, actor='Al Pacino'),\n",
       " Row(search_id=4, doc_id=215, actor='Adam Sandler'),\n",
       " Row(search_id=4, doc_id=219, actor='Anthony Hopkins'),\n",
       " Row(search_id=4, doc_id=89, actor='Angelina Jolie'),\n",
       " Row(search_id=4, doc_id=37, actor='Arnold Schwarzenegger'),\n",
       " Row(search_id=4, doc_id=78, actor='Al Pacino'),\n",
       " Row(search_id=5, doc_id=167, actor='Angelina Jolie'),\n",
       " Row(search_id=5, doc_id=17, actor='Amy Adams'),\n",
       " Row(search_id=5, doc_id=33, actor='Arnold Schwarzenegger'),\n",
       " Row(search_id=5, doc_id=207, actor='Arnold Schwarzenegger'),\n",
       " Row(search_id=5, doc_id=10, actor='Amy Adams'),\n",
       " Row(search_id=5, doc_id=213, actor='Adam Sandler'),\n",
       " Row(search_id=5, doc_id=140, actor='Anthony Hopkins'),\n",
       " Row(search_id=5, doc_id=14, actor='Amy Adams'),\n",
       " Row(search_id=5, doc_id=70, actor='Adam Sandler'),\n",
       " Row(search_id=5, doc_id=211, actor='Anthony Hopkins'),\n",
       " Row(search_id=6, doc_id=54, actor='Al Pacino'),\n",
       " Row(search_id=6, doc_id=112, actor='Anne Hathaway'),\n",
       " Row(search_id=6, doc_id=184, actor='Anne Hathaway'),\n",
       " Row(search_id=6, doc_id=212, actor='Angelina Jolie'),\n",
       " Row(search_id=6, doc_id=116, actor='Al Pacino'),\n",
       " Row(search_id=6, doc_id=108, actor='Anne Hathaway'),\n",
       " Row(search_id=6, doc_id=213, actor='Adam Sandler'),\n",
       " Row(search_id=6, doc_id=175, actor='Adam Sandler'),\n",
       " Row(search_id=6, doc_id=204, actor='Adam Sandler'),\n",
       " Row(search_id=7, doc_id=149, actor='Amy Adams'),\n",
       " Row(search_id=7, doc_id=146, actor='Angelina Jolie'),\n",
       " Row(search_id=7, doc_id=153, actor='Arnold Schwarzenegger'),\n",
       " Row(search_id=7, doc_id=104, actor='Adam Sandler'),\n",
       " Row(search_id=7, doc_id=179, actor='Angelina Jolie'),\n",
       " Row(search_id=7, doc_id=2, actor='Adam Sandler'),\n",
       " Row(search_id=7, doc_id=195, actor='Adam Sandler'),\n",
       " Row(search_id=7, doc_id=70, actor='Adam Sandler'),\n",
       " Row(search_id=7, doc_id=109, actor='Al Pacino'),\n",
       " Row(search_id=7, doc_id=16, actor='Anthony Hopkins'),\n",
       " Row(search_id=8, doc_id=26, actor='Adam Sandler'),\n",
       " Row(search_id=8, doc_id=203, actor='Anthony Hopkins'),\n",
       " Row(search_id=8, doc_id=71, actor='Al Pacino'),\n",
       " Row(search_id=8, doc_id=111, actor='Angelina Jolie'),\n",
       " Row(search_id=8, doc_id=100, actor='Anne Hathaway'),\n",
       " Row(search_id=8, doc_id=213, actor='Adam Sandler'),\n",
       " Row(search_id=8, doc_id=162, actor='Adam Sandler'),\n",
       " Row(search_id=8, doc_id=175, actor='Adam Sandler'),\n",
       " Row(search_id=8, doc_id=75, actor='Anthony Hopkins'),\n",
       " Row(search_id=8, doc_id=173, actor='Adam Sandler'),\n",
       " Row(search_id=9, doc_id=54, actor='Al Pacino'),\n",
       " Row(search_id=9, doc_id=77, actor='Al Pacino'),\n",
       " Row(search_id=9, doc_id=68, actor='Anthony Hopkins'),\n",
       " Row(search_id=9, doc_id=101, actor='Al Pacino'),\n",
       " Row(search_id=9, doc_id=171, actor='Al Pacino'),\n",
       " Row(search_id=9, doc_id=154, actor='Amy Adams'),\n",
       " Row(search_id=9, doc_id=175, actor='Adam Sandler'),\n",
       " Row(search_id=9, doc_id=18, actor='Amy Adams'),\n",
       " Row(search_id=9, doc_id=195, actor='Adam Sandler'),\n",
       " Row(search_id=9, doc_id=169, actor='Adam Sandler'),\n",
       " Row(search_id=10, doc_id=203, actor='Anthony Hopkins'),\n",
       " Row(search_id=10, doc_id=103, actor='Amy Adams'),\n",
       " Row(search_id=10, doc_id=28, actor='Arnold Schwarzenegger'),\n",
       " Row(search_id=10, doc_id=185, actor='Amy Adams'),\n",
       " Row(search_id=10, doc_id=179, actor='Angelina Jolie'),\n",
       " Row(search_id=10, doc_id=142, actor='Anthony Hopkins'),\n",
       " Row(search_id=10, doc_id=210, actor='Amy Adams'),\n",
       " Row(search_id=10, doc_id=160, actor='Anthony Hopkins'),\n",
       " Row(search_id=10, doc_id=194, actor='Al Pacino'),\n",
       " Row(search_id=10, doc_id=165, actor='Anne Hathaway')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_results = []\n",
    "for i, result in enumerate(ten_results):\n",
    "    aggregated_results += [(i+1, doc_id) for doc_id, vector in result]\n",
    "\n",
    "agg_df = sc.parallelize(aggregated_results).toDF([\"search_id\", \"doc_id\"])\n",
    "agg_df.join(sanitized, agg_df.doc_id == sanitized.id).select(\"search_id\", \"doc_id\", \"actor\").orderBy(\"search_id\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5 - Actors as categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>actor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Adam Sandler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>Adam Sandler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>Adam Sandler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>Adam Sandler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>Adam Sandler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>201</td>\n",
       "      <td>Arnold Schwarzenegger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>207</td>\n",
       "      <td>Arnold Schwarzenegger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>208</td>\n",
       "      <td>Arnold Schwarzenegger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>221</td>\n",
       "      <td>Arnold Schwarzenegger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>224</td>\n",
       "      <td>Arnold Schwarzenegger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>226 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                  actor\n",
       "0      2           Adam Sandler\n",
       "1     12           Adam Sandler\n",
       "2     24           Adam Sandler\n",
       "3     26           Adam Sandler\n",
       "4     30           Adam Sandler\n",
       "..   ...                    ...\n",
       "221  201  Arnold Schwarzenegger\n",
       "222  207  Arnold Schwarzenegger\n",
       "223  208  Arnold Schwarzenegger\n",
       "224  221  Arnold Schwarzenegger\n",
       "225  224  Arnold Schwarzenegger\n",
       "\n",
       "[226 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanitized.select(\"id\", \"actor\").orderBy([\"actor\"]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT actor)|\n",
      "+---------------------+\n",
      "|                    7|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as func\n",
    "\n",
    "sanitized.select(func.countDistinct(\"actor\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Means\n",
    "We have 7 actors, thats 7 groups. I expect 7 distinct clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# vectors is an rdd of (index(doc id), vector)\n",
    "def kmeans(vectors, K):\n",
    "    centers =randomly_pick_centers(vectors, K)\n",
    "    change_center = True\n",
    "    while change_center:\n",
    "        change_center = False\n",
    "        closest_centers = vectors.map(lambda v: (find_nearest_center(v[1], centers), (v[1], 1, v[0])))\n",
    "        pointStats = closest_centers.reduceByKey(lambda v1, v2: (v1[0] + v2[0], v1[1] + v2[1]))\n",
    "        newPoints = pointStats.map(lambda st: (st[0], st[1][0] / st[1][1])).collect()\n",
    "        \n",
    "        for (iK, v) in newPoints:\n",
    "            if v != centers[iK]:\n",
    "                centers[iK] = v\n",
    "                change_center = True\n",
    "    return closest_centers # (index of center, (vector, 1, doc id))\n",
    "\n",
    "def find_nearest_center(v1, centers):\n",
    "    best_center_index = 0\n",
    "    closest_center = float('inf')\n",
    "    for i, c in enumerate(centers):\n",
    "        current_distance = np.sum((v1 - c[1]) ** 2)\n",
    "        current_distance = 1\n",
    "        if current_distance < closest_center:\n",
    "            closest_center = current_distance\n",
    "            best_center_index = i\n",
    "    return best_center_index\n",
    "            \n",
    "\n",
    "def randomly_pick_centers(vectors, centers_amount):\n",
    "    \"\"\"\n",
    "    Uses Pyspark's RDD \"takeSample\" methid which returns a fixed-size sampled subset\n",
    "    of an RDD.\n",
    "        note:: This method should only be used if the resulting array is expected\n",
    "            to be small, as all the data is loaded into the driver's memory.\n",
    "            \n",
    "    Here I'm using it as we are dealing with a relatively small data set and its an \n",
    "    Academic project\n",
    "    \"\"\"\n",
    "    return vectors.takeSample(False, centers_amount, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 1668.0 failed 1 times, most recent failure: Lost task 4.0 in stage 1668.0 (TID 56102, efa9f4a5b0af, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 595, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 425, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1946, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-66-ec1a2db0b70f>\", line 10, in <lambda>\n  File \"<ipython-input-66-ec1a2db0b70f>\", line 24, in find_nearest_center\nTypeError: unsupported operand type(s) for ** or pow(): 'DenseVector' and 'int'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor141.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 595, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 425, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1946, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-66-ec1a2db0b70f>\", line 10, in <lambda>\n  File \"<ipython-input-66-ec1a2db0b70f>\", line 24, in find_nearest_center\nTypeError: unsupported operand type(s) for ** or pow(): 'DenseVector' and 'int'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-020533afec0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtester\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtester\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-ec1a2db0b70f>\u001b[0m in \u001b[0;36mkmeans\u001b[0;34m(vectors, K)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mclosest_centers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfind_nearest_center\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mpointStats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosest_centers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mnewPoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpointStats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnewPoints\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \"\"\"\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 1668.0 failed 1 times, most recent failure: Lost task 4.0 in stage 1668.0 (TID 56102, efa9f4a5b0af, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 595, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 425, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1946, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-66-ec1a2db0b70f>\", line 10, in <lambda>\n  File \"<ipython-input-66-ec1a2db0b70f>\", line 24, in find_nearest_center\nTypeError: unsupported operand type(s) for ** or pow(): 'DenseVector' and 'int'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor141.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 595, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 425, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1946, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-66-ec1a2db0b70f>\", line 10, in <lambda>\n  File \"<ipython-input-66-ec1a2db0b70f>\", line 24, in find_nearest_center\nTypeError: unsupported operand type(s) for ** or pow(): 'DenseVector' and 'int'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "tester = kmeans(sc.parallelize([(i,v) for i, v in enumerate(vectors_list)]), 7)\n",
    "tester.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37727"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(randomly_pick_centers(sc.parallelize([(i,v) for i, v in enumerate(vectors_list)]), 7)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
